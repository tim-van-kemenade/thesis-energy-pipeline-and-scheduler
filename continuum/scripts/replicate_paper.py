"""\
Run the benchmark multiple times with a range of settings,
and produce tables / graphs with these results
"""

import math
import time
import argparse
import sys
import logging
import subprocess
import datetime
import os

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import numpy as np
import pandas as pd

from matplotlib.ticker import ScalarFormatter
from matplotlib.ticker import NullFormatter

# Home dir should be continuum/
os.chdir("../")


def enable_logging(verbose):
    """Enable logging"""
    # Set parameters
    new_level = logging.INFO
    if verbose:
        new_level = logging.DEBUG

    new_format = "[%(asctime)s %(filename)20s:%(lineno)4s - %(funcName)25s() ] %(message)s"
    logging.basicConfig(format=new_format, level=new_level, datefmt="%Y-%m-%d %H:%M:%S")

    logging.info("Logging has been enabled")


def execute(command, shell=False, crash=True):
    """Execute a process using the subprocess library,
    and return the output/error or the process

    Args:
        command (list(str) or str): Command to be executed.
        shell (bool). Optional. Execute subprocess via shell. Defaults to False.
        crash (bool). Optional. Crash on error. Defaults to True

    Returns:
        (list(str), list(str)): Return the output and error generated by this process.
    """
    if isinstance(command, str):
        logging.info(command)
    else:
        logging.info(" ".join(command))

    output = []
    error = []

    try:
        with subprocess.Popen(
            command, shell=shell, stdout=subprocess.PIPE, stderr=subprocess.PIPE
        ) as process:
            output = [line.decode("utf-8") for line in process.stdout.readlines()]
            error = [line.decode("utf-8") for line in process.stderr.readlines()]
    except Exception as _:
        if crash:
            sys.exit()

    return output, error


class Experiment:
    """Experiment template / super class"""

    def __init__(self, resume):
        self.resume = resume
        self.runs = []

    def check_resume(self):
        """If the resume argument is given, get the first x log files >= the resume date,
        and use their output instead of re-running the experiment.
        """
        if self.resume is None:
            return

        log_location = "./logs"
        logs = [f for f in os.listdir(log_location) if f.endswith(".log")]
        logs.sort()
        exp_i = 0

        for log in logs:
            splits = log.split("_")
            dt = splits[0] + "_" + splits[1]
            dt = datetime.datetime.strptime(dt, "%Y-%m-%d_%H:%M:%S")

            if dt >= self.resume:
                path = os.path.join(log_location, log)
                logging.info("File %s for experiment run %i", path, exp_i)

                with open(path, "r", encoding="utf-8") as f:
                    output = f.readlines()
                    f.close()

                self.runs[exp_i]["output"] = output
                exp_i += 1

                # We have all logs needed
                if exp_i == len(self.runs):
                    break

    def run_commands(self):
        """Execute all generated commands"""
        for run in self.runs:
            if run["command"] == []:
                continue

            # Skip runs where we got output with --resume
            if run["output"] is not None:
                logging.info("Skip command: %s", " ".join(run["command"]))
                continue

            output, error = execute(run["command"])

            logging.debug("------------------------------------")
            logging.debug("OUTPUT")
            logging.debug("------------------------------------")
            logging.debug("\n%s", "".join(output))

            if error != []:
                logging.debug("------------------------------------")
                logging.debug("ERROR")
                logging.debug("------------------------------------")
                logging.debug("\n%s", "".join(error))
                sys.exit()

            logging.debug("------------------------------------")

            # Get output from log file
            logpath = output[0].rstrip().split("and file ")[-1]
            with open(logpath, "r", encoding="utf-8") as f:
                output = f.readlines()
                run["output"] = output
                f.close()


class EndpointScaling(Experiment):
    """Experiment:
    System load with an increasing number of endpoints connected to a singleworker

    So:
    - Run with all 3 deployment modes
    - Vary number of endpoints connected to a single worker
    """

    def __init__(self, resume):
        Experiment.__init__(self, resume)

        self.modes = ["cloud", "edge", "endpoint"]
        self.cores = [4, 2, 1]
        self.endpoints = [1, 2, 4, 8]

        self.y = None
        self.y_load = None
        self.y_latency = None

    def __repr__(self):
        """Returns this string when called as print(object)"""
        return """
APP                     image-classification
MODES                   %s
WORKERS                 1
CLOUD_CORES             %i
EDGE_CORES              %i
ENDPOINT_CORES          %i
ENDPOINTS/WORKER        %s""" % (
            ",".join(self.modes),
            self.cores[0],
            self.cores[1],
            self.cores[2],
            ",".join([str(endpoint) for endpoint in self.endpoints]),
        )

    def generate(self):
        """Generate commands to run the benchmark based on the current settings"""
        # Differ in deployment modes
        for mode in self.modes:
            if mode == "cloud":
                config = "cloud_endpoint"
                cores = self.cores[0]
            elif mode == "edge":
                config = "edge_endpoint"
                cores = self.cores[1]
            else:
                config = "endpoint"
                cores = self.cores[2]

            # Differ in #endpoints per worker
            for endpoint in self.endpoints:
                # No sense to use more than 1 endpoint in endpoint-only deployment mode
                if mode == "endpoint" and endpoint > 1:
                    continue

                command = [
                    "python3",
                    "continuum.py",
                    "-v",
                    "configuration/experiment_endpoint_scaling/" + config + str(endpoint) + ".cfg",
                ]
                command = [str(c) for c in command]

                run = {
                    "mode": mode,
                    "cores": cores,
                    "endpoints": endpoint,
                    "command": command,
                    "output": None,
                    "worker_time": None,
                }
                self.runs.append(run)

    def parse_output(self):
        """For all runs, get the worker runtime"""
        for run in self.runs:
            # Get the line containing the metrics
            i = -10
            for i, line in enumerate(run["output"]):
                if "Output in csv format" in line:
                    break

            # Get output based on type of run
            if run["mode"] != "endpoint":
                worker = run["output"][i + 1][1:-4]
                endpoint = run["output"][i + 2][1:-4]
            else:
                worker = run["output"][i + 1][1:-4]
                endpoint = run["output"][i + 1][1:-4]

            # Get worker output, parse into dataframe
            w1 = [x.split(",") for x in worker.split("\\n")]
            w2 = [sub[1:] for sub in w1]
            wdf = pd.DataFrame(w2[1:], columns=w2[0])
            wdf["proc_time/data (ms)"] = pd.to_numeric(wdf["proc_time/data (ms)"], downcast="float")

            # Get endpoint output, parse into dataframe
            e1 = [x.split(",") for x in endpoint.split("\\n")]
            e2 = [sub[1:] for sub in e1]
            edf = pd.DataFrame(e2[1:], columns=e2[0])
            edf["latency_avg (ms)"] = pd.to_numeric(edf["latency_avg (ms)"], downcast="float")

            # For worker, get the number of images processed per second across all cores
            processed_rate = wdf["proc_time/data (ms)"].mean()
            processed_rate = 1000.0 / processed_rate
            processed_rate *= run["cores"]

            # Calculate the generated number of images per second by endpoints
            frequency = 5
            requested_rate = float(frequency * run["endpoints"])

            # Calculate the utilization by comparing the generated workload to
            # the time it took to process the workload
            run["usage"] = int((requested_rate / processed_rate) * 100)

            # For endpoint, report the average end-to-end latency
            run["latency"] = int(edf["latency_avg (ms)"].mean())

    def plot(self):
        """Plot the results from executed runs - endpoint scaling"""
        # set width of bar
        plt.rcParams.update({"font.size": 22})
        _, ax1 = plt.subplots(figsize=(12, 6))
        ax2 = ax1.twinx()

        bar_width = 0.2
        bars = np.arange(len(self.modes))

        colors = ["dimgray", "gray", "darkgray", "lightgray"]

        y_total_load = []
        y_total_latency = []
        xs = []
        for endpoint, color in zip(self.endpoints, colors):
            # Get the x and y data
            y = [run["usage"] for run in self.runs if run["endpoints"] == endpoint]
            x = [x + math.log2(endpoint) * bar_width for x in bars]

            # mode=endpoint only uses 1 endpoint
            if endpoint > 1:
                x = x[:-1]

            # Plot the bar
            if not xs:
                # Only label for the first bar
                ax1.bar(
                    x,
                    y,
                    color=color,
                    width=bar_width * 0.9,
                    label="System Load",
                )
            else:
                ax1.bar(x, y, color=color, width=bar_width * 0.9, label="System Load")

            y_total_load += y

            # For the latency line plot
            y = [run["latency"] for run in self.runs if run["endpoints"] == endpoint]
            y_total_latency += y
            xs += x

        # Plot latency line
        ys = y_total_latency
        xs, ys = zip(*sorted(zip(xs, ys)))
        ax2.plot(
            xs[:4],
            ys[:4],
            color="midnightblue",
            linewidth=3.0,
            marker="o",
            markersize=12,
            label="End-to-end latency",
        )
        ax2.plot(
            xs[4:8],
            ys[4:8],
            color="midnightblue",
            linewidth=3.0,
            marker="o",
            markersize=12,
            label="End-to-end latency",
        )
        ax2.plot(
            xs[8],
            ys[8],
            color="midnightblue",
            linewidth=3.0,
            marker="o",
            markersize=12,
            label="End-to-end latency",
        )

        # Add horizontal lines every 100 percent
        ax1.axhline(y=100, color="k", linestyle="-", linewidth=3)
        ax1.axhline(y=200, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=300, color="k", linestyle="-", linewidth=1, alpha=0.5)

        label_ticks = [
            i + j * bar_width
            for i, j in [
                (0, 0),
                (0, 1),
                (0, 2),
                (0, 3),
                (1, 0),
                (1, 1),
                (1, 2),
                (1, 3),
                (2, 0),
            ]
        ]
        ax1.set_xticks(label_ticks, ["1", "2", "4", "8", "1", "2", "4", "8", "1"])

        # Set y axis 1: load
        h1, l1 = ax1.get_legend_handles_labels()
        h2, l2 = ax2.get_legend_handles_labels()
        ax1.legend([h1[0]] + [h2[0]], [l1[0]] + [l2[0]], loc="upper left", framealpha=1.0)

        ax1.set_ylabel("System Load")
        ax1.yaxis.set_major_formatter(mtick.PercentFormatter())
        ax1.set_ylim(0, 400)
        ax1.set_yticks(np.arange(0, 500, 100))

        # Set y axis 2: latency
        ax2.set_ylabel("End-to-end latency (ms)")
        ax2.set_yscale("log")
        ax2.set_ylim(100, 1000000)

        # Save
        t = time.strftime("%Y-%m-%d_%H:%M:%S", time.gmtime())
        plt.savefig("./logs/EndpointScaling_load_%s.pdf" % (t), bbox_inches="tight")

        self.y_load = y_total_load
        self.y_latency = y_total_latency

    def print_result(self):
        """Print results of runs as text"""
        i = 0
        for endpoint in self.endpoints:
            for mode in self.modes:
                if mode == "endpoint" and endpoint > 1:
                    break

                logging.info(
                    "Mode: %10s | Endpoints: %3s | System Load: %4i%% | Latency: %10i ms",
                    mode,
                    endpoint,
                    self.y_load[i],
                    self.y_latency[i],
                )
                i += 1


class Deployments(Experiment):
    """Experiment:
    Run large scale cloud, edge, and endpoint deployments
    """

    def __init__(self, resume):
        Experiment.__init__(self, resume)

        self.config_path = "configuration/experiment_large_deployments/"
        self.configs = [
            "cloud.cfg",
            "edge_large.cfg",
            "edge_small.cfg",
            "mist.cfg",
        ]
        self.modes = ["cloud", "edge", "edge", "edge"]
        self.cores = [4, 4, 2, 2]
        self.endpoints = [4, 4, 2, 1]

        self.y = None
        self.y_load = None
        self.y_latency = None

    def __repr__(self):
        """Returns this string when called as print(object)"""
        return """
APP                     image-classification
CONFIGS                 %s""" % (
            ",".join([str(config) for config in self.configs]),
        )

    def generate(self):
        """Generate commands to run the benchmark based on the current settings"""
        for config, mode, core, endpoint in zip(
            self.configs, self.modes, self.cores, self.endpoints
        ):
            command = ["python3", "continuum.py", "-v", self.config_path + config]
            command = [str(c) for c in command]

            run = {
                "mode": mode,
                "cores": core,
                "endpoints": endpoint,
                "config": config.split(".")[0],
                "command": command,
                "output": None,
                "worker_time": None,
            }
            self.runs.append(run)

    def parse_output(self):
        """For all runs, get the worker runtime"""
        for run in self.runs:
            # Get the line containing the metrics
            i = -10
            for i, line in enumerate(run["output"]):
                if "Output in csv format" in line:
                    break

            # Get output based on type of run
            if run["mode"] != "endpoint":
                worker = run["output"][i + 1][1:-4]
                endpoint = run["output"][i + 2][1:-4]
            else:
                worker = run["output"][i + 1][1:-4]
                endpoint = run["output"][i + 1][1:-4]

            # Get worker output, parse into dataframe
            w1 = [x.split(",") for x in worker.split("\\n")]
            w2 = [sub[1:] for sub in w1]
            wdf = pd.DataFrame(w2[1:], columns=w2[0])
            wdf["proc_time/data (ms)"] = pd.to_numeric(wdf["proc_time/data (ms)"], downcast="float")
            wdf["delay_avg (ms)"] = pd.to_numeric(wdf["delay_avg (ms)"], downcast="float")

            # Get endpoint output, parse into dataframe
            e1 = [x.split(",") for x in endpoint.split("\\n")]
            e2 = [sub[1:] for sub in e1]
            edf = pd.DataFrame(e2[1:], columns=e2[0])
            edf["latency_avg (ms)"] = pd.to_numeric(edf["latency_avg (ms)"], downcast="float")
            edf["preproc_time/data (ms)"] = pd.to_numeric(
                edf["preproc_time/data (ms)"], downcast="float"
            )

            # For worker, get the number of images processed per second across all cores
            processed_rate = wdf["proc_time/data (ms)"].min()
            processed_rate = 1000.0 / processed_rate
            processed_rate *= run["cores"]

            # Calculate the generated number of images per second by endpoints
            frequency = 5
            requested_rate = float(frequency * run["endpoints"])

            # Calculate the utilization by comparing the generated workload to
            # the time it took to process the workload
            run["usage"] = int((requested_rate / processed_rate) * 100)

            # For endpoint, report the average end-to-end latency
            run["latency"] = int(edf["latency_avg (ms)"].min())

            # Save breakdown of latency
            run["latency_breakdown"] = [
                int(wdf["proc_time/data (ms)"].min()),
                int(
                    edf["latency_avg (ms)"].min()
                    - edf["preproc_time/data (ms)"].min()
                    - wdf["proc_time/data (ms)"].min()
                ),
            ]

    def plot(self):
        """Plot the results from executed runs - compare large-scale deployments"""
        # set width of bar
        plt.rcParams.update({"font.size": 22})
        _, ax1 = plt.subplots(figsize=(12, 6))

        bar_width = 0.2

        y_total_load = []
        y_total_latency = []
        xs = []

        y = [run["usage"] for run in self.runs]
        x = [x * bar_width for x in range(len(self.configs))]

        ax1.bar(
            x,
            y,
            color="dimgray",
            width=bar_width * 0.9,
            label="System Load",
        )

        y_total_load += y

        # For the latency line plot
        y = [run["latency"] for run in self.runs]
        y_total_latency += y
        xs += x

        # Add horizontal lines every 100 percent
        ax1.axhline(y=100, color="k", linestyle="-", linewidth=3)
        ax1.axhline(y=75, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=50, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=25, color="k", linestyle="-", linewidth=1, alpha=0.5)

        ax1.set_xticks(x, ["Cloud", "Edge-Large", "Edge-Small", "Mist"])
        ax1.set_xlabel("Deployment")

        # Set y axis 1: load
        ax1.set_ylabel("System Load")
        ax1.yaxis.set_major_formatter(mtick.PercentFormatter())
        ax1.set_ylim(0, 100)
        ax1.set_yticks(np.arange(0, 125, 25))

        # Save
        t = time.strftime("%Y-%m-%d_%H:%M:%S", time.gmtime())
        plt.savefig("./logs/LargeDeployments_load_%s.pdf" % (t), bbox_inches="tight")

        self.y_load = y_total_load
        self.y_latency = y_total_latency

        self.plot2()

    def plot2(self):
        """Plot the results from executed runs - breakdown in computation vs communication"""
        _, ax = plt.subplots(figsize=(12, 6))

        colors = ["dimgray", "lightgray"]
        bars = ["Cloud", "Edge-Large", "Edge-Small", "Mist"]
        stacks = ["Processing", "Communication"]

        raw_vals = [run["latency_breakdown"] for run in self.runs]
        ax.bar(
            bars,
            list(list(zip(*raw_vals))[0]),
            color=colors[0],
            label=stacks[0],
        )
        ax.bar(
            bars,
            list(list(zip(*raw_vals))[1]),
            color=colors[1],
            label=stacks[1],
            bottom=list(list(zip(*raw_vals))[0]),
        )

        ax.set_ylabel("Time (ms)")
        ax.set_ylim(0, 400)
        ax.set_xlabel("Deployment")
        ax.legend(loc="upper left", framealpha=1.0)
        t = time.strftime("%Y-%m-%d_%H:%M:%S", time.gmtime())
        plt.savefig("./logs/LargeDeployments_breakdown_%s.pdf" % (t), bbox_inches="tight")

    def print_result(self):
        """Print results of runs as text"""
        for i, config in enumerate(self.configs):
            logging.info(
                "Conf: %15s | Sys Load: %4i%% | Latency: %10i ms | Comp: %10i ms | Comm: %10i ms",
                config,
                self.y_load[i],
                self.y_latency[i],
                self.runs[i]["latency_breakdown"][0],
                self.runs[i]["latency_breakdown"][1],
            )


class LatencyCPUVariation(Experiment):
    """Experiment:
    Deploy 1 cloud worker and 1 endpoint, and vary latency from 0ms to 100ms in steps of 10
    Also vary the CPU cores / memory from 0.5 (GB) to 8.0 (GB)

    So:
    - Run with minimal cloud deployment
    - Change latency from 0ms to 100ms between cloud and endpoint in steps of 10ms
    - Change application CPU and memory from 0.5 to 8.0 in power of 2 steps
    """

    def __init__(self, resume):
        Experiment.__init__(self, resume)

        self.latency = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

        # CPU x quota: 0.5 | 1.0 | 2.0 | 4.0 | 8.0
        self.cpu = [1, 1, 2, 4, 8]
        self.memory = [0.5, 1.0, 2.0, 4.0, 8.0]
        self.quota = [0.5, 1.0, 1.0, 1.0, 1.0]

        self.y = None

    def __repr__(self):
        """Returns this string when called as print(object)"""
        return """
APP                     image-classification
LATENCY                 %s
CPU CORES               %s
MEMORY (GB)             %s
QUOTA                   %s""" % (
            ",".join(str(latency) for latency in self.latency),
            ",".join(str(cpu) for cpu in self.cpu),
            ",".join(str(memory) for memory in self.memory),
            ",".join(str(quota) for quota in self.quota),
        )

    def generate(self):
        """Generate commands to run the benchmark based on the current settings"""
        # Differ in deployment modes
        for latency in self.latency:
            for cpu, memory, quota in zip(self.cpu, self.memory, self.quota):
                cpu_quota = cpu * quota
                if cpu_quota != memory:
                    logging.error("ERROR: cpu x quota (%f) != memory (%f)", cpu_quota, memory)
                    sys.exit()

                if cpu_quota >= 1:
                    cpu_str = "%s00" % (int(cpu_quota))
                elif cpu_quota < 1:
                    cpu_str = "0%s" % (int(cpu_quota * 100))

                config = "cloud_%ims_cpu%s.cfg" % (latency, cpu_str)
                command = [
                    "python3",
                    "continuum.py",
                    "-v",
                    "configuration/experiment_latency_variation/" + config,
                ]
                command = [str(c) for c in command]

                run = {
                    "cpu_quota_memory": cpu_quota,
                    "network_latency": latency,
                    "command": command,
                    "output": None,
                    "latency": None,
                }
                self.runs.append(run)

    def parse_output(self):
        """For all runs, get the worker runtime"""
        for run in self.runs:
            # Get the line containing the metrics
            i = -10
            for i, line in enumerate(run["output"]):
                if "Output in csv format" in line:
                    break

            # Get output based on type of run
            endpoint = run["output"][i + 2][1:-4]

            # Get endpoint output, parse into dataframe
            e1 = [x.split(",") for x in endpoint.split("\\n")]
            e2 = [sub[1:] for sub in e1]
            edf = pd.DataFrame(e2[1:], columns=e2[0])
            edf["latency_avg (ms)"] = pd.to_numeric(edf["latency_avg (ms)"], downcast="float")

            # For endpoint, report the average end-to-end latency
            run["latency"] = int(edf["latency_avg (ms)"].mean())

    def plot(self):
        """Plot the results from executed runs"""
        # set width of bar
        plt.rcParams.update({"font.size": 22})
        _, ax1 = plt.subplots(figsize=(12, 6))

        x = [run["network_latency"] for run in self.runs if run["cpu_quota_memory"] == 0.5]

        for cpu, quota in zip(self.cpu, self.quota):
            cpu_quota = cpu * quota
            y = [run["latency"] for run in self.runs if run["cpu_quota_memory"] == cpu_quota]

            ax1.plot(
                x,
                y,
                linewidth=3.0,
                marker="o",
                markersize=12,
                label="CPUs: %s" % (str(cpu_quota)),
            )

        # Set y axis: latency
        ax1.set_ylabel("End-to-end latency (ms)")
        ax1.set_yscale("log")
        ax1.set_ylim(100, 1000000)
        ax1.set_yticks([100, 1000, 10000, 100000, 1000000])

        ax1.set_xlabel("Network latency between cloud and endpoint (ms)")
        ax1.set_xlim(0, 100)
        ax1.set_xticks(np.arange(0, 110, 10))

        ax1.legend(loc="center", framealpha=1.0, ncol=3, bbox_to_anchor=[0.5, 0.37])

        ax1.axhline(y=1000, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=10000, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=100000, color="k", linestyle="-", linewidth=1, alpha=0.5)

        # Save
        t = time.strftime("%Y-%m-%d_%H:%M:%S", time.gmtime())
        plt.savefig("./logs/LatencyCPUVariation_%s.pdf" % (t), bbox_inches="tight")

    def print_result(self):
        """Print results of runs as text"""
        for run in self.runs:
            logging.info(
                "Network Latency: %5i ms | \
CPU Cores x Quota == Memory: %5f | \
End-to-end Latency: %5i ms",
                run["network_latency"],
                run["cpu_quota_memory"],
                run["latency"],
            )


class Provider(Experiment):
    """Experiment:
    Deploy a simple cloud experiment on QEMU and GCP

    So:
    - Run with minimal cloud deployment
    - Deploy on QEMU and GCP (and eventually bare-metal locally)
    """

    def __init__(self, resume):
        Experiment.__init__(self, resume)

        self.providers = ["qemu", "gcp"]
        self.modes = ["cloud", "edge", "mist", "endpoint"]

    def __repr__(self):
        """Returns this string when called as print(object)"""
        return """
APP                     image-classification
PROVIDERS               %s
MODES                   %s""" % (
            ",".join(self.providers),
            ",".join(self.modes),
        )

    def generate(self):
        """Generate commands to run the benchmark based on the current settings"""
        # Differ in deployment modes
        for provider in self.providers:
            for mode in self.modes:
                config = "%s_%s.cfg" % (provider, mode)
                command = [
                    "python3",
                    "continuum.py",
                    "-v",
                    "configuration/experiment_provider/" + config,
                ]
                command = [str(c) for c in command]

                run = {
                    "provider": provider,
                    "mode": mode,
                    "command": command,
                    "output": None,
                    "latency_breakdown": None,
                }
                self.runs.append(run)

    def parse_output(self):
        """For all runs, get the worker runtime"""
        for run in self.runs:
            # Get the line containing the metrics
            i = -10
            for i, line in enumerate(run["output"]):
                if "Output in csv format" in line:
                    break

            # Get output based on type of run
            if run["mode"] != "endpoint":
                worker = run["output"][i + 1][1:-4]
                endpoint = run["output"][i + 2][1:-4]
            else:
                worker = run["output"][i + 1][1:-4]
                endpoint = run["output"][i + 1][1:-4]

            # Get worker output, parse into dataframe
            w1 = [x.split(",") for x in worker.split("\\n")]
            w2 = [sub[1:] for sub in w1]
            wdf = pd.DataFrame(w2[1:], columns=w2[0])
            wdf["proc_time/data (ms)"] = pd.to_numeric(wdf["proc_time/data (ms)"], downcast="float")

            if run["mode"] != "endpoint":
                wdf["delay_avg (ms)"] = pd.to_numeric(wdf["delay_avg (ms)"], downcast="float")

            # Get endpoint output, parse into dataframe
            e1 = [x.split(",") for x in endpoint.split("\\n")]
            e2 = [sub[1:] for sub in e1]
            edf = pd.DataFrame(e2[1:], columns=e2[0])
            edf["latency_avg (ms)"] = pd.to_numeric(edf["latency_avg (ms)"], downcast="float")

            if run["mode"] != "endpoint":
                edf["preproc_time/data (ms)"] = pd.to_numeric(
                    edf["preproc_time/data (ms)"], downcast="float"
                )

            # Save breakdown of latency
            if run["mode"] != "endpoint":
                # 1. Preprocessing (ignored because too small)
                # 2. Endpoint -> Offload target
                # 3. Queue
                # 4. Processing
                # 5. Offload target -> Endpoint
                processing = wdf["proc_time/data (ms)"].min()
                network_to_end = (
                    edf["latency_avg (ms)"].min()
                    - edf["preproc_time/data (ms)"].min()
                    - wdf["proc_time/data (ms)"].min()
                    - wdf["delay_avg (ms)"].min()
                )
                queue = wdf["delay_avg (ms)"].min() - network_to_end

                run["latency_breakdown"] = [
                    int(network_to_end * 2),
                    int(queue),
                    int(processing),
                ]
            else:
                # 1. Queue
                # 2. Processing
                queue = edf["latency_avg (ms)"].min() - wdf["proc_time/data (ms)"].min()
                processing = wdf["proc_time/data (ms)"].min()
                run["latency_breakdown"] = [0, int(queue), int(processing)]

    def plot(self):
        """Plot the results from executed runs - breakdown in computation vs communication"""
        _, ax = plt.subplots(figsize=(12, 6))

        offset = 0.2
        xmin = [i - offset for i in range(len(self.modes))]
        xplus = [i + offset for i in range(len(self.modes))]

        colors = ["black", "dimgrey", "lightgrey"]

        raw_vals_qemu = [run["latency_breakdown"] for run in self.runs if run["provider"] == "qemu"]
        q_comm = list(list(zip(*raw_vals_qemu))[0])
        q_queue = list(list(zip(*raw_vals_qemu))[1])
        q_proc = list(list(zip(*raw_vals_qemu))[2])
        ax.bar(
            xmin,
            q_comm,
            color=colors[0],
            width=2 * offset,
            label="QEMU communication",
            edgecolor="midnightblue",
        )
        ax.bar(
            xmin,
            q_queue,
            color=colors[1],
            bottom=q_comm,
            width=2 * offset,
            label="QEMU queue",
            edgecolor="midnightblue",
        )
        ax.bar(
            xmin,
            q_proc,
            color=colors[2],
            bottom=[c + q for c, q in zip(q_comm, q_queue)],
            width=2 * offset,
            label="QEMU processing",
            edgecolor="midnightblue",
        )

        raw_vals_gcp = [run["latency_breakdown"] for run in self.runs if run["provider"] == "gcp"]
        g_comm = list(list(zip(*raw_vals_gcp))[0])
        g_queue = list(list(zip(*raw_vals_gcp))[1])
        g_proc = list(list(zip(*raw_vals_gcp))[2])

        # TMP bugfix
        g_comm = [q + g for q, g in zip(q_comm, g_comm)]

        ax.bar(
            xplus,
            g_comm,
            color=colors[0],
            width=2 * offset,
            label="GCP communication",
            hatch="/",
            edgecolor="midnightblue",
        )
        ax.bar(
            xplus,
            g_queue,
            color=colors[1],
            bottom=g_comm,
            width=2 * offset,
            label="GCP queue",
            hatch="/",
            edgecolor="midnightblue",
        )
        ax.bar(
            xplus,
            g_proc,
            color=colors[2],
            bottom=[c + q for c, q in zip(g_comm, g_queue)],
            width=2 * offset,
            label="GCP processing",
            hatch="/",
            edgecolor="midnightblue",
        )

        ax.set_ylabel("End-to-end latency (ms)")
        ax.set_ylim(0, 300)

        ax.set_xlabel("Deployment")
        bars = ["Cloud", "Edge", "Mist", "Endpoint"]
        ax.set_xticks(np.arange(len(bars)), labels=bars)

        ax.legend(loc="upper right", ncol=2, framealpha=1.0)
        t = time.strftime("%Y-%m-%d_%H:%M:%S", time.gmtime())
        plt.savefig("./logs/Providers_%s.pdf" % (t), bbox_inches="tight")

    def print_result(self):
        """Print results of runs as text"""
        comm = [0, 0, 0, 0]
        for i, provider in enumerate(self.providers):
            for j, mode in enumerate(self.modes):
                index = (i * len(self.providers)) + j
                # TMP fix
                if provider == "gcp":
                    self.runs[index]["latency_breakdown"][0] = (
                        comm[j] + self.runs[index]["latency_breakdown"][0]
                    )

                logging.info(
                    "Provider: %6s| Mode: %8s | Comm: %5i ms | Queue: %5i ms | Comp: %5i ms",
                    provider,
                    mode,
                    self.runs[index]["latency_breakdown"][0],
                    self.runs[index]["latency_breakdown"][1],
                    self.runs[index]["latency_breakdown"][2],
                )

                comm[j] = self.runs[index]["latency_breakdown"][0]


class Serverless(Experiment):
    """Experiment:
    Change #endpoints and CPU/mem per worker to test serverless

    So:
    - Scale #endpoints per (always 1) worker from 1 to 8
    - Scale #cpu_cores and memory (GB) from 2 to 8
    """

    def __init__(self, resume):
        Experiment.__init__(self, resume)

        self.cpu = [2, 4, 8]
        self.endpoints = [1, 2, 4, 8]

        self.y = None

    def __repr__(self):
        """Returns this string when called as print(object)"""
        return """
APP                     image-classification
WORKERS                 1
CLOUD_CORES             %s
ENDPOINTS/WORKER        %s""" % (
            ",".join([str(c) for c in self.cpu]),
            ",".join([str(endpoint) for endpoint in self.endpoints]),
        )

    def generate(self):
        """Generate commands to run the benchmark based on the current settings"""
        for cores in self.cpu:
            for endpoints in self.endpoints:
                config = "openfaas_endpoint%s_cpu%s.cfg" % (endpoints, cores)

                command = [
                    "python3",
                    "continuum.py",
                    "-v",
                    "configuration/experiment_serverless/" + config,
                ]
                command = [str(c) for c in command]

                run = {
                    "cpu": cores,
                    "endpoints": endpoints,
                    "command": command,
                    "output": None,
                    "latency": None,
                }
                self.runs.append(run)

    def parse_output(self):
        """For all runs, get the worker runtime"""
        for run in self.runs:
            # Get the line containing the metrics
            i = -10
            for i, line in enumerate(run["output"]):
                if "Output in csv format" in line:
                    break

            # Get output based on type of run
            endpoint = run["output"][i + 2][1:-4]

            # Get endpoint output, parse into dataframe
            e1 = [x.split(",") for x in endpoint.split("\\n")]
            e2 = [sub[1:] for sub in e1]
            edf = pd.DataFrame(e2[1:], columns=e2[0])
            edf["latency_avg (ms)"] = pd.to_numeric(edf["latency_avg (ms)"], downcast="float")

            # For endpoint, report the average end-to-end latency
            run["latency"] = int(edf["latency_avg (ms)"].mean())

    def plot(self):
        """Plot the results from executed runs"""
        # set width of bar
        plt.rcParams.update({"font.size": 22})
        _, ax1 = plt.subplots(figsize=(12, 6))

        x = self.endpoints

        colors = ["green", "red", "purple"]

        for cpu, color in zip(self.cpu, colors):
            y = [run["latency"] for run in self.runs if run["cpu"] == cpu]
            self.y = y

            ax1.plot(
                x,
                y,
                linewidth=3.0,
                marker="o",
                markersize=12,
                color=color,
                label="CPUs: %s" % (str(cpu)),
            )

        # Set y axis: latency
        ax1.set_ylabel("End-to-end latency (ms)")
        ax1.set_ylim(900, 2600)
        ax1.set_yticks([1000, 1500, 2000, 2500])

        ax1.set_xlabel("Endpoints connected per worker")
        ax1.set_xscale("log")
        ax1.set_xlim(1, 8)
        ax1.set_xticks([1, 2, 4, 8])
        ax1.xaxis.set_major_formatter(ScalarFormatter())
        ax1.xaxis.set_minor_formatter(NullFormatter())

        ax1.legend(loc="upper left", framealpha=1.0)

        ax1.axhline(y=500, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=1000, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=1500, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=2000, color="k", linestyle="-", linewidth=1, alpha=0.5)
        ax1.axhline(y=2500, color="k", linestyle="-", linewidth=1, alpha=0.5)

        # Save
        t = time.strftime("%Y-%m-%d_%H:%M:%S", time.gmtime())
        plt.savefig("./logs/Serverless_%s.pdf" % (t), bbox_inches="tight")

    def print_result(self):
        """Print results of runs as text"""
        for run in self.runs:
            logging.info(
                "#Endpoints: %5i | CPU Cores / Memory (GB): %s | End-to-end Latency: %5i ms",
                run["endpoints"],
                run["cpu"],
                run["latency"],
            )


def main(args):
    """Main function

    Args:
        args (Namespace): Argparse object
    """
    if args.experiment == "EndpointScaling":
        logging.info("Experiment: Scale endpoint connect to a single worker")
        exp = EndpointScaling(args.resume)
    elif args.experiment == "Deployments":
        logging.info("Experiment: Change deployments between cloud, edge, and local")
        exp = Deployments(args.resume)
    elif args.experiment == "LatencyCPUVariation":
        logging.info("Experiment: Vary latency between cloud and endpoint")
        exp = LatencyCPUVariation(args.resume)
    elif args.experiment == "Provider":
        logging.info("Experiment: Vary infrastructure providers")
        exp = Provider(args.resume)
    elif args.experiment == "Serverless":
        logging.info("Experiment: Vary #endpoints and CPU/Memory per worker for Serverless")
        exp = Serverless(args.resume)
    else:
        logging.error("Invalid experiment: %s", args.experiment)
        sys.exit()

    logging.info(exp)
    exp.generate()
    exp.check_resume()
    exp.run_commands()
    exp.parse_output()
    exp.plot()
    exp.print_result()


if __name__ == "__main__":
    # Get input arguments and parse them
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "experiment",
        choices=[
            "EndpointScaling",
            "Deployments",
            "LatencyCPUVariation",
            "Provider",
            "Serverless",
        ],
        help="Experiment to replicate",
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="increase verbosity level")
    parser.add_argument(
        "-r",
        "--resume",
        type=lambda s: datetime.datetime.strptime(s, "%Y-%m-%d_%H:%M:%S"),
        help='Resume a previous Experiment from datetime "YYYY-MM-DD_HH:mm:ss"',
    )
    arguments = parser.parse_args()

    enable_logging(arguments.verbose)
    main(arguments)
